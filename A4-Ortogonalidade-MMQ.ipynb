{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ortogonalidade e MMQ\n",
    "A introdução de um produto escalar $\\langle \\cdot, \\cdot \\rangle$ em um espaço vetorial $V$ de dimensão finita $n$, além de determinar uma distância entre os vetores, também define o ângulo. Lembremos que:\n",
    "$$ \\| x-y \\| = \\sqrt{\\langle x-y,x-y \\rangle}.$$\n",
    "Para o conceito de ângulo, vamos começar só com ortoganalidade. Dizemos que dois vetores $x$ e $y$ são **ortogonais** quando o produto interno entre eles for zero, $\\langle x, y\\rangle = 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usamos o numpy para a o produto escalar usual\n",
    "import numpy as np\n",
    "x=np.array([1,2,0,4,0])\n",
    "y=np.array([2,-1,4,0,2])\n",
    "# np.dot() é a função de produto escalar para vetores ver help(np.dot)para o caso geral\n",
    "np.dot(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processo de ortogonalização de Gram-Schmidt\n",
    "A partir de uma base qualquer $\\{\\mathbf{e}_1,\\dots,\\mathbf{e}_n\\}$, podemos construir uma base ortogonal (os elementos são, dois a dois, ortogonais) $\\{\\mathbf{f}_1,\\dots,\\mathbf{f}_n\\}$, num processo iterativo chamado *método de Gram-Schmidt*:\n",
    "$$ \\begin{eqnarray}\n",
    "\\mathbf{f}_1 &= &\\mathbf{e}_1\\\\\n",
    "\\mathbf{f}_k & = & \\mathbf{e}_k - \\sum_{i=1}^{k-1}\\frac{\\langle \\mathbf{e}_k,\\mathbf{f}_i\\rangle}{\\langle \\mathbf{f}_i,\\mathbf{f}_i\\rangle}\\mathbf{f}_i\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GramSchimdt(E):\n",
    "    '''Aplica o processo de Gram-Schmidt para uma array de vetores '''\n",
    "    n,k = np.shape(E)\n",
    "    F = E.copy()\n",
    "    for i in range(1,k):\n",
    "        F[:,i]=F[:,i]-sum(np.dot(E[:,i],F[:,j])/np.dot(F[:,j],F[:,j])*F[:,j] for j in range(i))\n",
    "    return F        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# Teste da função:\n",
    "# Exemplo 1\n",
    "E = np.array([[2,1],[1,0],[0,2]], dtype=float)\n",
    "print(E)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.   0.2]\n",
      " [ 1.  -0.4]\n",
      " [ 0.   2. ]]\n"
     ]
    }
   ],
   "source": [
    "F=GramSchimdt(E)\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  1.  0.]\n",
      " [ 1. -1.  3.]\n",
      " [ 0.  2.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 2\n",
    "E1 = np.array([[2,1,0],[1,-1,3],[0,2,1]], dtype=float)\n",
    "print(E1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.          0.6        -1.03448276]\n",
      " [ 1.         -1.2         2.06896552]\n",
      " [ 0.          2.          1.55172414]]\n"
     ]
    }
   ],
   "source": [
    "F1=GramSchimdt(E1)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complemento ortogonal\n",
    "Se $W \\subset V$ é um subspaço vetorial de dimensão $k$ então o conjunto $W^\\bot =\\{ y \\in V: \\langle x, y \\rangle =0 \\forall x\\in W\\}$ é o complemento ortogonal de $W$. É um subespaço de dimensão $n-k$ e $V=W\\oplus W^\\bot$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* De fato. Tome uma base ortogonal qualquer de $W$, $\\{a_1,\\dots,a_k\\}$.\n",
    "* Para cada $v\\in V$ definimos $\\hat{v} = \\sum_{i=1}^k \\frac{ \\langle v, a_i \\rangle}{\\langle a_i, a_i \\rangle}a_i$. Chamado de projeção ortogonal de $v$ em $W$.\n",
    "* O operador linear $T(v)= v-\\hat{v}$ tem o núcleo $\\ker{T} = W$ e a imagem  $\\text{Im}(T) = W^\\bot$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## propriedade de minimização da projeção ortogonal\n",
    "Seja $W$ um subespaço de $V$, de dimensão $k$. Para cada $v\\in V$, denotamos por $\\hat{v}$ a projeção ortogonal de $v$ em $W$.\n",
    "Vimos no parágrafo anterior que $(v-\\hat{v})\\in W^\\bot$. Para um outro $w$ qualquer de $W$, podemos escrever $w=\\hat{v} + w_1$.\n",
    "Daí:\n",
    "$$ \\|v-w\\|^2 = \\|v-\\hat{v} -w_1\\|^2 = \\langle v-\\hat{v} -w_1,v-\\hat{v} -w_1\\rangle = \\|v-\\hat{v}\\|^2 + \\|w_1\\|^2 $$\n",
    "Então a projeção ortogonal $\\hat{v}$ é o vetor de $W$ mais próximo de $v$ pela norma euclidiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
